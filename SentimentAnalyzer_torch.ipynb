{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap to GPU if available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "with open(\"/data/reviews.txt\", \"r\") as f:\n",
    "    reviews = f.read()\n",
    "with open(\"/data/labels.txt\", \"r\") as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "full_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews_split = full_text.split(\"\\n\")\n",
    "full_text = ' '.join(reviews_split)\n",
    "words = full_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabulary and mapping to int\n",
    "\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting reviews to int\n",
    "\n",
    "reviews_ints = [[vocab_to_int[word] for word in review.split()]\n",
    "                for review in reviews_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing labels\n",
    "\n",
    "labels_split = labels.split(\"\\n\")\n",
    "labels = np.array([1 if label == \"positive\" else 0 for label in labels_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering reviews\n",
    "\n",
    "reviews_ints = [review for review in reviews_ints if len(review) > 0]\n",
    "labels = labels[:len(reviews_ints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "\n",
    "seq_len = 200\n",
    "features = pad_sequences(reviews_ints, maxlen=seq_len,\n",
    "                         padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data (train, evaluate, test)\n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features) * split_frac)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x) * 0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to PyTorch tensors\n",
    "\n",
    "train_x, train_y = torch.tensor(train_x), torch.tensor(train_y)\n",
    "val_x, val_y = torch.tensor(val_x), torch.tensor(val_y)\n",
    "test_x, test_y = torch.tensor(test_x), torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-set: 20000\n",
      "Test-set: 2500\n",
      "Evaluation-set: 2500\n"
     ]
    }
   ],
   "source": [
    "# Data size\n",
    "\n",
    "print(f\"Training-set: {len(train_x)}\")\n",
    "print(f\"Test-set: {len(test_x)}\")\n",
    "print(f\"Evaluation-set: {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "\n",
    "batch_size = 500\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "val_data = TensorDataset(val_x, val_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "\n",
    "\n",
    "class MainNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=300, lstm_size=256, seq_len=200):\n",
    "        super(MainNetwork, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size + 1, embedding_dim=embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv1d = nn.Conv1d(in_channels=embed_size,\n",
    "                                out_channels=128, kernel_size=3)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # LSTM layer (bidirectional)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=128, hidden_size=lstm_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Second LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=lstm_size * 2,\n",
    "                            hidden_size=lstm_size, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # Pooling global layer\n",
    "        self.global_maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(lstm_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embed_size)\n",
    "        # Change shape for Conv1D: (batch_size, embed_size, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Convolutional layer + ReLU + Maxpool\n",
    "        # Shape after conv and maxpool: (batch_size, 128, (seq_len - 2) // 2)\n",
    "        x = self.maxpool(F.relu(self.conv1d(x)))\n",
    "\n",
    "        # LSTM (bidirectional)\n",
    "        # Shape after permute: (batch_size, (seq_len - 2) // 2, lstm_size * 2)\n",
    "        x, _ = self.bilstm(x.permute(0, 2, 1))\n",
    "\n",
    "        # Second LSTM layer\n",
    "        # Shape: (batch_size, (seq_len - 2) // 2, lstm_size)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = self.global_maxpool(x.permute(0, 2, 1)).squeeze(\n",
    "            2)  # Shape: (batch_size, lstm_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # Output layer\n",
    "        x = torch.sigmoid(self.fc4(x))  # Shape: (batch_size, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function and optimizer\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = MainNetwork(vocab_size=vocab_size).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Train loss: 0.694531310, Train accuracy: 49.84%\n",
      "Val accuracy: 50.00%\n",
      "Val loss: 0.693643045\n",
      "Test loss: 0.693660438\n",
      "Test accuracy: 50.00%\n",
      "Epoch [2/15], Train loss: 0.693395093, Train accuracy: 50.81%\n",
      "Val accuracy: 50.00%\n",
      "Val loss: 0.692884409\n",
      "Test loss: 0.692963636\n",
      "Test accuracy: 50.00%\n",
      "Epoch [3/15], Train loss: 0.693049328, Train accuracy: 50.57%\n",
      "Val accuracy: 63.12%\n",
      "Val loss: 0.690353537\n",
      "Test loss: 0.690864432\n",
      "Test accuracy: 62.28%\n",
      "Epoch [4/15], Train loss: 0.680482647, Train accuracy: 58.71%\n",
      "Val accuracy: 63.08%\n",
      "Val loss: 0.670454681\n",
      "Test loss: 0.675945270\n",
      "Test accuracy: 61.08%\n",
      "Epoch [5/15], Train loss: 0.669698909, Train accuracy: 62.70%\n",
      "Val accuracy: 67.24%\n",
      "Val loss: 0.643294692\n",
      "Test loss: 0.650564039\n",
      "Test accuracy: 65.08%\n",
      "Epoch [6/15], Train loss: 0.630415747, Train accuracy: 67.78%\n",
      "Val accuracy: 67.88%\n",
      "Val loss: 0.612734997\n",
      "Test loss: 0.624853337\n",
      "Test accuracy: 67.04%\n",
      "Epoch [7/15], Train loss: 0.585210395, Train accuracy: 71.95%\n",
      "Val accuracy: 73.28%\n",
      "Val loss: 0.550111043\n",
      "Test loss: 0.563193083\n",
      "Test accuracy: 72.00%\n",
      "Epoch [8/15], Train loss: 0.534804627, Train accuracy: 75.91%\n",
      "Val accuracy: 74.80%\n",
      "Val loss: 0.515217507\n",
      "Test loss: 0.513342011\n",
      "Test accuracy: 75.48%\n",
      "Epoch [9/15], Train loss: 0.485992517, Train accuracy: 79.36%\n",
      "Val accuracy: 77.60%\n",
      "Val loss: 0.482749283\n",
      "Test loss: 0.487848538\n",
      "Test accuracy: 77.04%\n",
      "Epoch [10/15], Train loss: 0.449895247, Train accuracy: 81.39%\n",
      "Val accuracy: 78.68%\n",
      "Val loss: 0.467521220\n",
      "Test loss: 0.477868772\n",
      "Test accuracy: 78.08%\n",
      "Epoch [11/15], Train loss: 0.406444325, Train accuracy: 83.82%\n",
      "Val accuracy: 78.20%\n",
      "Val loss: 0.478125548\n",
      "Test loss: 0.474479830\n",
      "Test accuracy: 78.60%\n",
      "Epoch [12/15], Train loss: 0.377048928, Train accuracy: 85.16%\n",
      "Val accuracy: 80.16%\n",
      "Val loss: 0.448905677\n",
      "Test loss: 0.457873189\n",
      "Test accuracy: 79.64%\n",
      "Epoch [13/15], Train loss: 0.342743595, Train accuracy: 87.34%\n",
      "Val accuracy: 80.60%\n",
      "Val loss: 0.447574168\n",
      "Test loss: 0.469024026\n",
      "Test accuracy: 80.12%\n",
      "Epoch [14/15], Train loss: 0.306789295, Train accuracy: 89.09%\n",
      "Val accuracy: 80.96%\n",
      "Val loss: 0.464290565\n",
      "Test loss: 0.489836568\n",
      "Test accuracy: 81.16%\n",
      "Epoch [15/15], Train loss: 0.295170562, Train accuracy: 89.24%\n",
      "Val accuracy: 81.44%\n",
      "Val loss: 0.485696805\n",
      "Test loss: 0.514694905\n",
      "Test accuracy: 81.04%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "val_accuracies = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    train_correct = 0  \n",
    "    train_total = 0  \n",
    "\n",
    "    for i, labels in train_loader:\n",
    "        i, labels = i.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(i)  # Pass the data through the model\n",
    "\n",
    "        # Output should have shape (batch_size, 1) and labels should have shape (batch_size,)\n",
    "        loss = criterion(output.view(-1), labels.float())  # Calculate the loss\n",
    "\n",
    "        loss.backward()  # Backpropagate to compute gradients\n",
    "        optimizer.step()  # Update the weights\n",
    "        train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "        # Calcular previsões e atualizar contadores\n",
    "        predictions = torch.round(output.view(-1))  # Previsões (0 ou 1)\n",
    "        # Contar previsões corretas\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        # Adicionar o tamanho do batch ao total de amostras\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    # Calcular precisão e perda média\n",
    "    train_accuracy = train_correct / train_total \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train loss: {train_loss / len(train_loader):.9f}, Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i, labels in val_loader:\n",
    "            i, labels = i.to(device), labels.to(device)  # Move data to GPU\n",
    "            output = model(i)  # Pass the data through the model\n",
    "            loss = criterion(output.view(-1), labels.float())  # Calculate the loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "            predictions = torch.round(output.view(-1))\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total \n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Val accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(f\"Val loss: {val_loss / len(val_loader):.9f}\")\n",
    "\n",
    "    # Evaluating the model (test data)\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i, labels in test_loader:\n",
    "            i, labels = i.to(device), labels.to(device)  # Move data to GPU\n",
    "            output = model(i)  # Pass the data through the model\n",
    "\n",
    "            # Calculate the loss\n",
    "            test_loss += criterion(output.view(-1), labels.float()).item()\n",
    "\n",
    "            predictions = torch.round(output.view(-1))  # Get predictions\n",
    "            # Count correct predictions\n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    test_accuracy = test_correct / test_total \n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Print test loss and accuracy\n",
    "    print(f\"Test loss: {test_loss / len(test_loader):.9f}\")\n",
    "    print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy curve graph plot\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_accuracies, label=\"Train accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Evaluation accuracy\")\n",
    "plt.title(\"Accuracy curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
